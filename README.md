# Gaussian Mixture Model

Determine the probability distribution of latent variables using a Gaussian Mixture Model. Compare results across mixture parameters like number of components and initialization type. Each program will plot and display the results, showing both mean AIC and BIC curves. 

### Dependencies

Scikit-learn

Scikit-learn requires:
	Python (>= 2.7 or >= 3.3),
	NumPy (>= 1.8.2),
	SciPy (>= 0.13.3).
(http://scikit-learn.org/stable/install.html)

### Initialization

Run both gmm_random.py and gmm_k_means.py and compare the resulting plots.

### Determination

We find that k-means and random initialization start around the same criterion, or error. Because lambda is constant, random initialization appears to be a constant of k. With k-means we can achieve higher accuracy with greater levels of complexity (2-3), but the error slowly increases across the BIC function. AIC remains relatively constant with the increased complexity. 

Conversely, the error of both AIC and BIC increase constantly with added complexity for random initialization. Like in k-means, though, BIC penalizes more for the additional complexity of the model. For this reason, BIC seems to be the better choice as it is more responsive to inputs like complexity when calculating the error rate. Three components would have likely been used if the original data were generated by a GMM. This is the point displays the lowest error â€“ the closest to being correct. 